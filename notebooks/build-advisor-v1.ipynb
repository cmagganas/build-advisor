{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load instructor and openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch the OpenAI client with Instructor\n",
    "client = instructor.from_openai(OpenAI(api_key=os.environ['OPENAI_API_KEY']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user query/proposal idea üí°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proposal = input(\"Please enter the ai app you'd like to develop and the problem you are trying to solve:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to create a portfolio chatbot that answers questions about my experience and qualifications for a role that I might be applying to\n"
     ]
    }
   ],
   "source": [
    "print(user_proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM #1 - Define Scope & Industry üè≠üóÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### business opportunity w/ scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM #2 - very critical (pass ‚úÖ or fail ‚ùå w/ explanation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM #3 - scope üõù & find existing workflows üîçüìá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### existing tools and workflows üîçüìá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG = \"\"\"\n",
    "Overview of Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG is a design pattern based on the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks from 2020. It is not a framework, product, or product category. RAG is particularly used in the development of AI applications, especially ‚ÄúQuestion-Answer‚Äù based applications (ChatBots). Over the past year, it has gained popularity due to the increasing demand for LLM-based applications. The primary goal of RAG is to provide more relevant answers based on private data, reducing hallucinations and ensuring that answers are current. By retrieving relevant information and providing it to the LLM to compose an answer, RAG improves efficiency and is scalable to use other knowledge sources.\n",
    "\n",
    "Growth and Integration of RAG Components\n",
    "With the growth in demand, both big tech companies and startups have been developing components for the RAG pipeline. These are often heterogeneous systems that allow integration with other vendors' components. For instance, LangChain (an orchestration tool) can be used with Vertex AI (Google‚Äôs AI framework). Numerous variants of the RAG pipeline exist, and we will explore a few below.\n",
    "\n",
    "Naive RAG\n",
    "This basic variant serves as a starting point for RAG implementation. The process begins by ingesting and chunking data. This is analogous to tearing pages out of an encyclopedia and then embedding these chunks into a vector database. Various databases, such as MongoDB, Pinecone, Weaviate, and Qdrant, offer these capabilities. The RAG pipeline retrieves relevant chunks of data using an embedding model and passes this information to the LLM, which then composes an answer.\n",
    "\n",
    "Key Steps:\n",
    "\t1.\tData Ingestion and Chunking: Breaking down data into manageable pieces.\n",
    "\t2.\tEmbedding: Converting chunks into numeric and storing in a vector database.\n",
    "\t3.\tRetrieval: Using an embedding model to fetch relevant data chunks.\n",
    "\t4.\tLLM Composition: LLM receives the retrieved data and generates the response.\n",
    "\n",
    "Configuration and Selection Considerations\n",
    "When setting up a RAG pipeline, various components and settings need to be considered:\n",
    "\t‚Ä¢\tText Ingest Tool\n",
    "\t‚Ä¢\tChunking Strategy\n",
    "\t‚Ä¢\tFoundation Model\n",
    "\t‚Ä¢\tEmbedding Model\n",
    "\t‚Ä¢\tModel Fine Tuning\n",
    "\t‚Ä¢\tVector Database and its Metadata\n",
    "\t‚Ä¢\tRetrieval Methods\n",
    "\t‚Ä¢\tPrompt Template\n",
    "\t‚Ä¢\tOrchestration Tools\n",
    "\t‚Ä¢\tEvaluation (Eval) Tools\n",
    "\t‚Ä¢\tLLMOps\n",
    "\t‚Ä¢\tUI Framework\n",
    "Component Candidates:\n",
    "\t‚Ä¢\tLLM: OpenAI, Anthropic, MetaAI, Mistral\n",
    "\t‚Ä¢\tEmbedding Model: OpenAI, Snowflake, Mistral\n",
    "\t‚Ä¢\tVector Database: Pinecone, Weaviate, MongoDB, Qdrant\n",
    "\t‚Ä¢\tOrchestration: LangChain, LlamaIndex, Haystack\n",
    "\t‚Ä¢\tFramework: Google Vertex AI, Amazon Bedrock, Microsoft Azure, \n",
    "\t\tOracle OCI, Databricks, Datastax, Snowflake\n",
    "\t‚Ä¢\tNo/Low Code: LangFlow, Fixie, Verta, Vectara, OpenAI GPTs\n",
    "\t‚Ä¢\tEvaluation: RAGas, LangSmith, Arize Phoenix, Quotient\n",
    "\n",
    "Some, like retrievers, can be selected based on the purpose of your application.  For example, LangChain provides a set of retrievers: Multi-Query Retriever.  Once it‚Äôs working, when can we go to the market? How does it get deployed and monitored?  What Responsible AI policies are necessary for your business?  Emerging evaluation, monitoring and observability tools are helping, but we should not rush to market unless the product will succeed. There is always going to balancing act between 1) Accuracy, 2) Performance 3) Latency\n",
    "\n",
    "Advanced RAG Variants\n",
    "Agentic RAG: Introduces a loop where the application can use tools and iterate retrieval until the necessary inference is provided. Suitable for complex, multi-step processes.\n",
    "\n",
    "Streaming RAG: Designed for dynamic data needs, such as providing real-time market updates. Uses tools like Bytewax with Kafka to provide continuous data streams to the vector database.\n",
    "\n",
    "AI Agents: Beyond information retrieval, AI agents can execute specific tasks autonomously. Applications can be created with multiple agents operating together, executing various tasks, including those using RAG.\n",
    "\n",
    "AI Product Evaluation Tool - AI App\n",
    "Currently, the development of Retrieval-Augmented Generation (RAG) applications and agents can appear opaque and challenging for product managers and business development professionals. The complexity and ambition inherent in AI application development often present significant hurdles.\n",
    "To address these challenges, we propose the creation of AI App, an advanced AI Agent Application designed to streamline and clarify the development process. AI App would generate comprehensive Product Requirements Documents (PRDs) and provide engineers with actionable recommendations, accurate delivery estimates, and insights into partner relations. Additionally, it would identify potential risks and contingencies.\n",
    "\n",
    "This tool represents a significant opportunity to enhance the planning and execution of RAG applications. By leveraging data-backed planning tools, AI App can ensure accurate estimations and provide clear guidance throughout the development lifecycle, ultimately facilitating more effective and efficient project management.\n",
    "\n",
    "\n",
    "Conclusion\n",
    "RAG is a powerful approach that, while not a silver bullet, significantly enhances the relevance and accuracy of AI-generated responses. The field is evolving with various configurations and tools that cater to different use cases. Understanding and utilizing these components effectively can drive the successful deployment of AI applications in the market.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing tools\n",
    "existing_tools = {\n",
    "    {\n",
    "        \"name\": \"LangChain\",\n",
    "        \"description\": \"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\",\n",
    "        \"link\": \"https://www.langchain.com/\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LlamaIndex\",\n",
    "        \"description\": \"LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).\",\n",
    "        \"link\": \"https://www.llamaindex.ai/\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"None\",\n",
    "        \"description\": \"Catch all for non-technical, out-of-scope or infeasible project ideas.\",\n",
    "        \"link\": None\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing techniques\n",
    "existing_techniques = {\n",
    "    {\n",
    "        \"name\": \"RAG\",\n",
    "        \"description\": \"RAG is a powerful approach that, while not a silver bullet, significantly enhances the relevance and accuracy of AI-generated responses. The field is evolving with various configurations and tools that cater to different use cases. Understanding and utilizing these components effectively can drive the successful deployment of AI applications in the market.\",\n",
    "        \"instruction\": RAG\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Yolo\",\n",
    "        \"description\": \"Just sent it!\",\n",
    "        \"instruction\": None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### search and match üîçüìá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define category\n",
    "sample_feasible_category = \"i have a dataset of jobs and candidates to match\"\n",
    "sample_infeasible_category = \"i have a jar of chocolate chip cookies\"\n",
    "\n",
    "# find most related workflow solution\n",
    "def most_related_workflow(category):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Proposal Hypothesis\n",
    "\n",
    "1. üß† User Idea\n",
    "2. Business Opportunity w/ Scope\n",
    "3. Business Opportunity w/ Scope & ‚úÖ\n",
    "\n",
    "‚¨áÔ∏è\n",
    "\n",
    "Hypothesis:\n",
    "\n",
    "- What the project is aiming to solve.\n",
    "- Why this is important.\n",
    "- Related solution implementation.\n",
    "- How it will implement:\n",
    "    - metrics to measure and monitor\n",
    "    - costs, time, other resources\n",
    "    - best practices for this implementation\n",
    "    - how this solution incorporates original idea's specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM #4 - Code Implementation ü§ñüß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_llm(proposal):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Solution w/ Explanation üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. code to try"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
